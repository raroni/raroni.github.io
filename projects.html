<!DOCTYPE html>
<html lang="en"><head><title>Project Log</title><meta charset="utf-8"></meta><link rel="stylesheet" href="/main.css"></link><link rel="stylesheet" href="/katex/katex.css"></link></head><body><h1>Project Log</h1><section><p>
      Here I maintain a log of software projects that I have worked on over the years. Some of these are hobby projects while others were made in an academic or professional context.
    </p></section><section><h2>1. Realtime GI Using ReSTIR And Irradiance Caching (2024)</h2><p>
      I have found ReSTIR to be a delightfully flexible toolbox. It offers so many tweakable knobs and in this project I wanted to see how far I could push it in terms of quality and performance. My goal was to implement multi-bounce diffuse in a way that would run well on efficiency-oriented GPUs, like those found in laptops and mobiles.
    </p><figure><video controls="true" style="max-height: 26rem"><source src="/projects/restir_screenspace.mp4" type="video/mp4"/>
</video><figcaption>Fig 1: The renderer samples sun light and local light by using only 1/8th ray per pixel.</figcaption></figure><p>
      By sharing new ReSTIR candidate samples across 4x4 pixel tiles and by using shadow mapping, my implementation fires at most 1/8th rays per screen space pixel.
      Within this tight budget I managed to sample both the sun and local lights (see figure 1). I achieved this by slightly reformulating <a href="https://research.nvidia.com/publication/2021-06_restir-gi-path-resampling-real-time-path-tracing">ReSTIR GI</a> to also take direct light from emissive triangles into account. I reduced noise by using a mixture PDF for the candidate samples that stochastically picks between BRDF sampling or explicit light sampling.
    </p><figure><video controls="true" style="max-height: 26rem"><source src="/projects/restir_cache.mp4" type="video/mp4"/>
</video><figcaption>Fig 2: A cascaded sparse voxel irradiance cache enables approximation of infinite bounces.</figcaption></figure><p>
      I approximated infinite (diffuse) bounces via a sparse cascaded voxel irradiance cache (see figure 2). The cache used a ranking system heavily inspired by <a href="https://github.com/EmbarkStudios/kajiya/blob/main/docs/gi-overview.md">Kajiya Renderer</a> to effectively avoid leaking. I achieved fast parallel voxel reallocation by arranging voxel data in <a href="https://twitter.com/raroni86/status/1818186308327535088">a ring buffer designed for time-sliced compaction</a>. I also found that the cache was useful in screenspace denoising: Instead of starting integration from zero (black) when surfaces are disoccluded, the denoiser queries the cache for a much better starting value.
    </p><p>
      The renderer was built directly on top of MacOS using Rust and Metal. Using the scene in figure 1, a full frame (including gbuffer, motion vectors, ReSTIR, denoising, etc.) takes approximately 10ms on a Macbook Air M1 (2.6 teraflops).
    </p></section><section><h2>2. Realtime Lightmapper (2022)</h2><p>
      In this hobby project I wanted to build a realtime GI solution for less powerful devices (e.g. laptops), and to explore realtime importance sampling techniques.
      I settled on a realtime lightmapper based on Metal&#x27;s intersection API.
    </p><figure><video controls="true" style="max-height: 26rem"><source src="/projects/realtime_lightmapper.mp4" type="video/mp4"/>
</video><figcaption>Fig 3: The renderer has support for dynamic lighting and dynamic geometry.</figcaption></figure><p>
      I used a modular design that can switch between integrator and filter implementations at runtime. In addition to a standard MC integrator, the renderer includes a path-guided integrator (figure 4), and a <a href="https://research.nvidia.com/publication/2021-06_restir-gi-path-resampling-real-time-path-tracing">ReSTIR integrator</a>.
    </p><figure><video controls="true" style="max-height: 26rem"><source src="/projects/realtime_lightmapper_path_guiding.mp4" type="video/mp4"/>
</video><figcaption>Fig 4: The path-guided integrator progressively learns which directions and emissives are important.</figcaption></figure><p>
      Inspired by <a href="https://www.youtube.com/watch?v=MyTOGHqyquU">Stachowiak</a>, the renderer captures short-term statistics (mean, variance) for each lightmap texel. This data enables you to approximate standard deviations, which are then used to clamp history buffers. This yields a temporal filter that is more stable and more reactive than a naive <a href="https://en.wikipedia.org/wiki/Moving_average">Exponential Moving Average</a> (at the cost of increased memory usage).
    </p><figure><video controls="true" style="max-height: 26rem"><source src="/projects/realtime_lightmapper_light_test.mp4" type="video/mp4"/>
</video><figcaption>Fig 5: The statistics-guided temporal filter is more stable and more reactive than standard EMA.</figcaption></figure><p>
      Lightmaps can be a pain but many issues can be addressed via careful implementations.
      I wrote a specialized lightmap rasterizer which among other things take bilinear filtering into account and approximates the ideal sample position within each lightmap texel.
      Inspired by <a href="https://www.ea.com/frostbite/news/precomputed-global-illumination-in-frostbite">Precomputed Global Illumination in Frostbite (Yuriy O&#x27;Donnell)</a>, I also added an adaptive chart packer (parallelized on GPU using the <a href="https://en.wikipedia.org/wiki/MapReduce">MapReduce</a> model), which guarantees no bleeding between charts while ensuring high texture utilization (figure 6).
    </p><figure><video controls="true" style="max-height: 26rem"><source src="/projects/realtime_lightmapper_allocation.mp4" type="video/mp4"/>
</video><figcaption>Fig 6: The packer adjusts lightmap resolution as needed and time-slices GPU work across multiple frames.</figcaption></figure></section><section><h2>3. Sparse Probe Surface Cache (2022)</h2><p>
      Inspired by <a href="https://github.com/EmbarkStudios/kajiya/blob/main/docs/gi-overview.md#irradiance-cache-055ms">the irradiance cache structure in Tomasz Stachowiak&#x27;s amazing Kajiya renderer</a>, we spent a Unity Hackweek prototyping an idea for an efficient probe cache.
    </p><p>
      The cache is fully GPU-driven and is sparse in the sense that it only allocate probes where they are needed (usually near surfaces).
      Once allocated, probes integrate irradiance into spherical harmonics. Probes that haven&#x27;t been requested for a number of frames, are automatically deallocated.
    </p><p>
      On low-end devices you may want to query the cache directly. On high-end devices, the cache can service a high quality final gather pass (similar to Epic&#x27;s Lumen). We had many more ideas we wanted to try out, but you can only do so much in a week.
    </p><figure><video controls="true" style="max-height: 26rem"><source src="/projects/sparse_probe_cache.mp4" type="video/mp4"/>
</video><figcaption>Fig 7: Our sparse probe cache prototype in action. Probes are allocated and deallocated dynamically.</figcaption></figure></section><section><h2>4. Realtime GI Using Surfels (2022)</h2><p>
      Inspired by <a href="https://www.ea.com/seed/news/seed-project-picapica">Project PICA PICA</a> and <a href="https://www.ea.com/seed/news/siggraph21-global-illumination-surfels">EA GIBS</a>, I implemented realtime GI based on surfels. One of the key benefits of this approach is that it doesn&#x27;t require UV mapping and that it works relatively well with most types of geometry:
      static, dynamic, skinned, high/low frequency. Another is that the surfel structure can sample itself which yields relatively cheap infinite light bounces.
    </p><p>
      I precompute surfel positions/normals at mesh import time. This means that no computation is spent on surfel placement at runtime and that we get light bounces even from surfaces which the camera hasn&#x27;t yet seen. The renderer was written in Rust/Metal and runs on a Macbook Air.
    </p><figure><video controls="true" style="max-height: 26rem"><source src="/projects/realtime_surfels.mp4" type="video/mp4"/>
</video><figcaption>Fig 8: Surfels are versatile but sampling them efficiently in a scalable way can be tricky.</figcaption></figure></section><section><h2>5. Realtime GI Using Surface Cache + Final Gather (2021)</h2><p>
      In this project I maintain a temporally integrated surface cache encoded as UV-mapped lightmaps. The fact that the cache can resample itself over time means that I can approximate infinite bounces using relatively few rays per frame.
      Inspired by <a href="https://docs.unrealengine.com/5.0/en-US/RenderingFeatures/Lumen/">Epic&#x27;s Lumen</a>, I do a screenspace final gather on top of the surface cache which is filtered temporally and spatially. The final gather affords camera-dependent resolution of secondary rays, something that would not have been possible using the surface cache alone.
    </p><p>
      The renderer supports dynamic lights and dynamic geometry. It is written in Rust and Apple Metal, and it runs smoothly on a Macbook Air M1 (without a discrete GPU).
    </p><figure><video controls="true" style="max-height: 26rem"><source src="/projects/surface_cache_city.mp4" type="video/mp4"/>
</video><figcaption>Fig 9: The UV-mapped surface cache works particularly well with low-poly scenes.</figcaption></figure><figure><video controls="true" style="max-height: 26rem"><source src="/projects/surface_cache_sponza.mp4" type="video/mp4"/>
</video><figcaption>Fig 10: A somewhat challenging scene with a small non-analytical light source.</figcaption></figure></section><section><h2>6. Realtime Constructive Solid Geometry Pathtracer (2021)</h2><p>
      A Constructive Solid Geometry realtime pathtracer that utilizes temporal and spatial filtering techniques to eliminate noise. In each frame each pixel samples the rendering equation integral (several bounces) and accumulates the results via a <a href="https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average">Moving Exponential Average</a>. The output is then denoised via my adaption of <a href="https://cg.ivd.kit.edu/publications/2017/svgf/svgf_preprint.pdf">SVGF</a>.
    </p><p>
      The pathtracer supports realtime changes to geometry and lighting. In addition to diffusely bounced light, I added support for volumetric fog (extinction + in-scattering), day/night cycle, simple color grading, and a subtle vignette effect. It runs at 60fps on a Macbook Air.
    </p><figure><video controls="true" style="max-height: 26rem"><source src="/projects/csg_pathtracer_animated.mp4" type="video/mp4"/>
</video><figcaption>Fig 11: Constructive Solid Geometry affords manipulations that are not trivial to do with polygons.</figcaption></figure><figure><video controls="true" style="max-height: 26rem"><source src="/projects/csg_pathtracer_sponza.mp4" type="video/mp4"/>
</video><figcaption>Fig 12: A minimalistic CSG remake of Sponza that showcases day/night cycle and volumetric fog.</figcaption></figure></section><section><h2>7. Realtime Sky Occlusion Using Voxel Tracing (2020)</h2><p>
      At a Unity hackweek our team made a <a href="https://www.minecraft.net/">Minecraft</a> clone. Inspired by <a href="https://www.teardowngame.com/">Teardown</a> I implemented realtime voxel traced sky occlusion on the GPU and integrated it into Unity&#x27;s Universal Render Pipeline.
    </p><p>
      We adopted a sparse data layout that significantly reduced memory usage by not storing anything in empty regions of the world.
    </p><figure><img src="/projects/vtao_demo.gif" style="max-height: 22rem"/>
<figcaption>Fig 13: Demonstration of the effect. I did not have time to do denoising.</figcaption></figure><figure><img src="/projects/vtao_comparison.gif" style="max-height: 22rem"/>
<figcaption>Fig 14: Comparison of Unity&#x27;s screen space ambient occlusion (SSAO) and our effect.</figcaption></figure></section><section><h2>8. Realtime GI Probes Using SDF Textures (2020)</h2><p>
      In this hobby project I used Monte Carlo integration to calculate irradiance probes across several frames.
      I automatically generated a 3D SDF texture to enable fast raytracing on the GPU.
      The probe data was encoded using a sphere-to-square octahedral projection to ensure efficient per pixel sampling during shading.
    </p><p>
      The renderer was written from scratch in Rust and Metal. It was heavily inspired by <a href="https://godotengine.org/article/godot-40-gets-sdf-based-real-time-global-illumination">SDFGI</a> (Linietsky) and <a href="http://jcgt.org/published/0008/02/01/">DDGI</a> (Majercik, Guertin, Nowrouzezahrai, McGuire).
    </p><figure><video controls="true" style="max-height: 26rem"><source src="/projects/sdfgi.mp4" type="video/mp4"/>
</video><figcaption>Fig 15: As the directional light moves, the irradiance probes update accordingly in realtime.</figcaption></figure></section><section><h2>9. Realtime Occlusion Probes Using SDF Textures (2020)</h2><p>
      At Unity Hackweek 2020 my group and I implemented GPU raymarching of signed distance fields stored as 3D textures. We used this to generate directional occlusion probes in realtime.
    </p><figure><img src="/projects/realtime_sdf_ao.gif" style="max-height: 24rem"/>
<figcaption>Fig 16: Occlusion probes are updated in realtime by raymarching a SDF 3D texture.</figcaption></figure></section><section><h2>10. Hobby CPU Pathtracer (2020)</h2><p>
      I started this project to solidify my understanding of pathtracing, BRDFs, and importance sampling.
      A few highlights:
    </p><ul><li>Written in Rust.</li><li>Parallel execution using <a href="https://crates.io/crates/crayon">crayon</a>.</li><li>Russian Roulette for path termination.</li><li>Deterministic.</li><li>Diffuse, dielectric, and specular materials.</li><li>Next Event Estimation.</li></ul><figure><img src="/projects/rust_pathtracer.jpg" style="max-height: 24rem"/>
<figcaption>Fig 17: Cornell box with dielectric, diffuse, and specular materials.</figcaption></figure></section><section><h2>11. Bachelor Thesis: Evaluation of Spherical Function Bases (2019)</h2><p>
      In realtime computer graphics we are often interested in compressing sets of spherical functions such as an irradiance field.
      In my thesis I evaluated and compared several known spherical function bases such as Spherical Harmonics, Spherical Gaussians and Ambient Cubes.
      The result was a set of recommendations about which encoding techniques to use for particular types of signals (irradiance, radiance, occlusion/visibility, etc.).
    </p><p>
      I received the maximum grade for my report and defence.
      You can read the thesis <a href="http://www.polylab.dk/projects/thesis.pdf">here</a>.
    </p><figure><img src="/projects/thesis_results.png" style="max-height: 18rem"/>
<figcaption>Fig 18: A graph from the report&#x27;s analysis section that shows RMAE vs space requirements for various bases.</figcaption></figure><figure><img src="/projects/thesis_sh.png" style="max-height: 16rem"/>
<figcaption>Fig 19: SH illustrations from the report&#x27;s theory section.</figcaption></figure></section><section><h2>12. Lightmap Denoising Using Machine Learning (2019)</h2><p>
      A machine learning-based denoiser can smooth out variance in lightmaps caused by low sample counts. This allows you to generate good-looking lightmaps much faster since you do not need to wait for convergence.</p><p>
      At Unity Hackweek 2019 my group and I ported the <a href="https://github.com/OpenImageDenoise/oidn">Intel Open Image Denoiser</a> to Unity&#x27;s <a href="https://docs.unity3d.com/Manual/com.unity.barracuda.html">Barracuda platform</a> which enabled it to run on the GPU. Our primary goal was to learn about machine learning denoising.
    </p><figure><img src="/projects/lightmap_denoising.jpg" style="max-height: 24rem"/>
<figcaption>Fig 20: A lightmapped scene before (top) and after (bottom) denoising.</figcaption></figure></section><section><h2>13. GPU (CUDA) Pathtracer With Adaptive Sampling (2018)</h2><p>
      For our final assignment in a course about parallel computation at university, my group and I wrote an adaptive GPU pathtracer written in C++/CUDA. The pathtracer detects converged pixels and removes them from the working set. Our primary focus was to make this detection and reduction logic as efficient as possible on modern GPUs.
    </p><figure><img src="/projects/cuda_pathtracer_video.gif" style="max-height: 32rem"/>
<figcaption>Fig 21: Rendering of animated directional light in GPU pathtracer.</figcaption></figure><figure><img src="/projects/cuda_pathtracer_variance.gif" style="max-height: 32rem"/>
<figcaption>Fig 22: Visualization of per-pixel convergence.</figcaption></figure></section><section><h2>14. Spatially Coherent Lightmaps in Unity (2018)</h2><p>
      Lightmap baking involves packing all lightmapped object into a set of lightmaps.
      I devised and implemented a stable packing algorithm that bundled object that were nearby in world space into the same lightmaps.
      The benefit of this is that it makes it possible to batch draw calls more efficiently at runtime.
    </p><p>Due to other priorities at Unity, this feature unfortunately never shipped.</p><figure><img src="/projects/spatially_coherent_packing.gif" style="max-height: 9999rem"/>
<figcaption>Fig 23: Before and after enabling spatially coherent packing (color denotes lightmap index).</figcaption></figure></section><section><h2>15. Explicit Shape Sampling in Unity&#x27;s Progressive Lightmapper (2018)</h2><p>For Unity Hackweek 2018 my group and I added explicit sampling of disk/sphere/line lights in Unity&#x27;s progressive lightmapper.</p><figure><img src="/projects/shape_sampling.gif" style="max-height: 26rem"/>
<figcaption>Fig 24: Unity&#x27;s progressive lightmapper with explicit sphere sampling.</figcaption></figure></section><section><h2>16. Lightmap Seam Stitching (2017)</h2><p>
      A known problem with <a href="https://en.wikipedia.org/wiki/Lightmap">lightmapping</a> is seam artifacts along the borders of the UV islands that are neighbours in object space but separated in lightmap space.
      To solve this problem in <a href="https://unity.com/">Unity</a>, I implemented a technique that &quot;stitches&quot; together the seams by performing a least square error minization over the border texels of the UV islands.
      My solution to this was heavily inspired by <a href="http://miciwan.com/SIGGRAPH2013/Lighting%20Technology%20of%20The%20Last%20Of%20Us.pdf">Naughty Dog</a> and <a href="https://www.sebastiansylvan.com/post/LeastSquaresTextureSeams/">Sebastian Sylvan</a>.
    </p><figure><img src="/projects/seam_stitching_sphere.gif" style="max-height: 22rem"/>
<figcaption>Fig 25: Seam stitching on lightmapped sphere.</figcaption></figure><figure><img src="/projects/seam_stitching_terrain.gif" style="max-height: 32rem"/>
<figcaption>Fig 26: Seam stitching on lightmapped terrain.</figcaption></figure></section><section><h2>17. Networked Lockstep Synchronization (2016)</h2><p>
      An implementation of the lockstep synchronization algorithm used in some types of networked games.
      By making the simulation (collision detection etc.) fully deterministic on the client, you only need to transmit player actions across the network (as opposed to continuously transmitting the full server state).
    </p><figure><img src="/projects/lockstep.gif" style="max-height: 32rem"/>
<figcaption>Fig 27: Deterministic simulation synchronized over TCP (original is 60fps).</figcaption></figure></section><section><h2>18. Hobby Engine (2015)</h2><p>
      For fun and educational purposes I wrote a game engine from scratch in C++/OpenGL.
      A few highlights:
    </p><ul><li>Platform abstraction layer. Current only MacOS backend is implemented.</li><li>Data-oriented design using the Entity/Component pattern.</li><li>Deferred renderer with Variance Shadow Mapping, Screen Space Ambient Occlusion, and day/night cycle system.</li><li>Includes physics, pathfinding, steering, skeletal animation, and simple behaviour tree system for AI.</li><li>State simulation and rendering are completely decoupled (allowing for deterministic updates).</li></ul><p>Source code is <a href="https://github.com/raroni/Flowstone">available on Github</a>.</p><figure><img src="/projects/hobby_engine_2015.gif" style="max-height: 32rem"/>
<figcaption>Fig 28: Sped-up day/night cycle in my hobby engine.</figcaption></figure></section></body></html>